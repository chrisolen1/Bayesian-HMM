{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Hierarchical Dirichlet Process Hidden Markov Model (HDPHMM).\n",
    "The HDPHMM object collects a number of observed emission sequences, and estimates\n",
    "latent states at every time point, along with a probability structure that ties latent\n",
    "states to emissions. This structure involves\n",
    "  + A starting probability, which dictates the probability that the first state\n",
    "  in a latent seqeuence is equal to a given symbol. This has a hierarchical Dirichlet\n",
    "  prior.\n",
    "  + A transition probability, which dictates the probability that any given symbol in\n",
    "  the latent sequence is followed by another given symbol. This shares the same\n",
    "  hierarchical Dirichlet prior as the starting probabilities.\n",
    "  + An emission probability, which dictates the probability that any given emission\n",
    "  is observed conditional on the latent state at the same time point. This uses a\n",
    "  Dirichlet prior.\n",
    "Fitting HDPHMMs requires MCMC estimation. MCMC estimation is thus used to calculate the\n",
    "posterior distribution for the above probabilities. In addition, we can use MAP\n",
    "estimation (for example) to fix latent states, and facilitate further analysis of a\n",
    "Chain.\n",
    "\"\"\"\n",
    "# Support typehinting.\n",
    "from __future__ import annotations\n",
    "from typing import Any, Union, Optional, Set, Dict, Iterable, List, Callable, Generator\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import terminaltables\n",
    "import tqdm\n",
    "import functools\n",
    "import multiprocessing\n",
    "import string\n",
    "from scipy import special, stats\n",
    "from sympy.functions.combinatorial.numbers import stirling\n",
    "from chain import Chain\n",
    "from utils import label_generator, dirichlet_process_generator, shrink_probabilities\n",
    "from warnings import catch_warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorthand for numeric types.\n",
    "Numeric = Union[int, float]\n",
    "\n",
    "# Oft-used dictionary initializations with shorthands.\n",
    "DictStrNum = Dict[Optional[str], Numeric]\n",
    "InitDict = DictStrNum\n",
    "DictStrDictStrNum = Dict[Optional[str], DictStrNum]\n",
    "NestedInitDict = DictStrDictStrNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typing.Dict[typing.Union[str, NoneType], typing.Dict[typing.Union[str, NoneType], typing.Union[int, float]]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NestedInitDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_sequences = [[7,6,3,53,45,8,75,109],[7,45,1,8,7,6,2,67]]\n",
    "emissions=None\n",
    "\n",
    "chains = [Chain(sequence) for sequence in emission_sequences]\n",
    "\n",
    "priors = {\n",
    "            \"alpha\": lambda: np.random.gamma(2, 2),\n",
    "            \"gamma\": lambda: np.random.gamma(3, 3),\n",
    "            \"alpha_emission\": lambda: np.random.gamma(2, 2),\n",
    "            \"gamma_emission\": lambda: np.random.gamma(3, 3),\n",
    "            \"kappa\": lambda: np.random.beta(1, 1)}\n",
    "\n",
    "hyperparameters = {param: prior() for param, prior in priors.items()}\n",
    "\n",
    "n_initial: InitDict\n",
    "n_emission: NestedInitDict\n",
    "n_transition: NestedInitDict\n",
    "n_initial = {None: 0}\n",
    "n_emission = {None: {None: 0}}\n",
    "n_transition = {None: {None: 0}}\n",
    "\n",
    "p_initial: InitDict\n",
    "p_emission: NestedInitDict\n",
    "p_transition: NestedInitDict\n",
    "p_initial = {None: 1}\n",
    "p_emission = {None: {None: 1}}\n",
    "p_transition = {None: {None: 1}}\n",
    "\n",
    "auxiliary_transition_variables: NestedInitDict\n",
    "beta_transition: InitDict\n",
    "beta_emission: InitDict\n",
    "auxiliary_transition_variables = {None: {None: 0}}\n",
    "beta_transition = {None: 1}\n",
    "beta_emission = {None: 1}\n",
    "\n",
    "if emissions is None:\n",
    "        emissions = functools.reduce(  # type: ignore\n",
    "                set.union, (set(c.emission_sequence) for c in chains), set()\n",
    "            )\n",
    "elif not isinstance(emissions, set):\n",
    "    raise ValueError(\"emissions must be a set\")\n",
    "    emissions = emissions  # type: ignore\n",
    "    states: Set[Optional[str]] = set()\n",
    "        \n",
    "_label_generator = label_generator(string.ascii_lowercase)\n",
    "\n",
    "\n",
    "def c() -> int:\n",
    "    \"\"\"\n",
    "    Number of chains in the HMM.\n",
    "    :return: int\n",
    "    \"\"\"\n",
    "    return len(chains)\n",
    "    \n",
    "\n",
    "def k() -> int:\n",
    "    \"\"\"\n",
    "    Number of latent states in the HMM currently.\n",
    "    :return: int\n",
    "    \"\"\"\n",
    "    return len(states)\n",
    "    \n",
    "\n",
    "def n() -> int:\n",
    "    \"\"\"\n",
    "    Number of unique emissions. If `emissions` was specified when the HDPHMM was\n",
    "    created, then this counts the number of elements in `emissions`. Otherwise,\n",
    "    counts the number of observed emissions across all emission sequences.\n",
    "    :return: int\n",
    "    \"\"\"\n",
    "    return len(emissions)  \n",
    "\n",
    "eps = 1e-02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_sequence = [[7,6,3,53,45,8,75,109],[7,45,1,8,7,6,2,67]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bayesian_hmm\n",
    "sequences = [[7,6,3,53,45,8,75,109],[7,45,1,8,7,6,2,67]]\n",
    "sequences = [[1,2,3],[4,5,6]]\n",
    "\n",
    "hmm = bayesian_hmm.HDPHMM(sequences, sticky=False)\n",
    "\n",
    "hmm.initialise(k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states:  {'e', 'a', 'i', 'g', 'j', 'b', 'f', 'h', 'c', 'd'} \n",
      "\n",
      "n_initial  {'e': 0, 'a': 0, None: 0, 'i': 0, 'g': 0, 'j': 0, 'b': 0, 'f': 1, 'h': 1, 'c': 0, 'd': 0} \n",
      "\n",
      "n_transition:  {'e': {'e': 0, 'a': 0, None: 0, 'i': 0, 'g': 0, 'j': 0, 'b': 0, 'f': 0, 'h': 0, 'c': 0, 'd': 0}, 'a': {'e': 0, 'a': 0, None: 0, 'i': 0, 'g': 0, 'j': 1, 'b': 0, 'f': 0, 'h': 0, 'c': 0, 'd': 0}, None: {'e': 0, 'a': 0, None: 0, 'i': 0, 'g': 0, 'j': 0, 'b': 0, 'f': 0, 'h': 0, 'c': 0, 'd': 0}, 'i': {'e': 0, 'a': 0, None: 0, 'i': 0, 'g': 0, 'j': 0, 'b': 0, 'f': 0, 'h': 0, 'c': 0, 'd': 0}, 'g': {'e': 0, 'a': 0, None: 0, 'i': 0, 'g': 0, 'j': 0, 'b': 0, 'f': 0, 'h': 0, 'c': 0, 'd': 0}, 'j': {'e': 0, 'a': 0, None: 0, 'i': 0, 'g': 0, 'j': 0, 'b': 0, 'f': 0, 'h': 0, 'c': 0, 'd': 0}, 'b': {'e': 1, 'a': 0, None: 0, 'i': 0, 'g': 0, 'j': 0, 'b': 0, 'f': 0, 'h': 0, 'c': 0, 'd': 0}, 'f': {'e': 0, 'a': 1, None: 0, 'i': 0, 'g': 0, 'j': 0, 'b': 0, 'f': 0, 'h': 0, 'c': 0, 'd': 0}, 'h': {'e': 0, 'a': 0, None: 0, 'i': 0, 'g': 0, 'j': 0, 'b': 1, 'f': 0, 'h': 0, 'c': 0, 'd': 0}, 'c': {'e': 0, 'a': 0, None: 0, 'i': 0, 'g': 0, 'j': 0, 'b': 0, 'f': 0, 'h': 0, 'c': 0, 'd': 0}, 'd': {'e': 0, 'a': 0, None: 0, 'i': 0, 'g': 0, 'j': 0, 'b': 0, 'f': 0, 'h': 0, 'c': 0, 'd': 0}} \n",
      "\n",
      "n_emission:  {'e': {1: 0, 2: 0, 3: 1, 4: 0, 5: 0, 6: 0}, 'a': {1: 0, 2: 0, 3: 0, 4: 0, 5: 1, 6: 0}, None: {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}, 'i': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}, 'g': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}, 'j': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 1}, 'b': {1: 0, 2: 1, 3: 0, 4: 0, 5: 0, 6: 0}, 'f': {1: 0, 2: 0, 3: 0, 4: 1, 5: 0, 6: 0}, 'h': {1: 1, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}, 'c': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}, 'd': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}} \n",
      "\n",
      "hyperparams:  {'alpha': 7.428326416938535, 'gamma': 2.81993351397953, 'alpha_emission': 1.5105578835164817, 'gamma_emission': 7.593845702365801, 'kappa': 0} \n",
      "\n",
      "p_initial:  {'e': 0.01934985681399189, 'a': 2.813561763534153e-05, 'i': 0.10873973228786472, 'g': 0.002051930772969472, 'j': 0.012805541404428888, 'b': 0.03804749095902686, 'f': 0.09445486426142775, 'h': 0.12160399823493653, 'c': 0.18227022830932713, 'd': 0.0029122974308169246, None: 0.41773592390757436} \n",
      "\n",
      "p_emission:  {None: {1: 0.03149450277353282, 2: 0.9345743903348692, 3: 0.0005762483237441111, 4: 1.5000949864634818e-09, 5: 2.579634247484635e-07, 6: 0.033354599104334144}, 'e': {1: 7.035015657862565e-05, 2: 0.0002343480795975675, 3: 0.8889542690612511, 4: 0.03339978251373195, 5: 0.0009777286056604285, 6: 0.07636352158318031}, 'a': {1: 0.8185260099328959, 2: 0.014404782993160234, 3: 0.03518961902280355, 4: 1.1058103996363662e-12, 5: 0.12288566311443962, 6: 0.008993924935594779}, 'i': {1: 0.006695754188595246, 2: 5.831800005537894e-07, 3: 8.271453684660652e-05, 4: 0.279643958697433, 5: 3.0499411490761795e-06, 6: 0.7135739394559755}, 'g': {1: 0.0008030502360998639, 2: 3.178442614030051e-08, 3: 0.00023075097198623165, 4: 1.901323016825537e-08, 5: 2.940924108412091e-05, 6: 0.9989367387531733}, 'j': {1: 0.05316016167248247, 2: 0.48712329680798594, 3: 0.0005618133114766419, 4: 3.1392226284239925e-08, 5: 0.06633880533927129, 6: 0.3928158914765572}, 'b': {1: 0.3300241204531095, 2: 0.46476406723222224, 3: 0.17508280733880505, 4: 0.0045003596610295, 5: 4.981070769006584e-05, 6: 0.025578834607143594}, 'f': {1: 0.11350029509610912, 2: 8.814204008091003e-06, 3: 1.9336400252477225e-05, 4: 0.2933361408053424, 5: 2.414347464323854e-05, 6: 0.5931112700196446}, 'h': {1: 0.23086837058500986, 2: 0.4382023908172365, 3: 0.10572584340442284, 4: 0.0001495675365066145, 5: 2.6809086410670404e-09, 6: 0.22505382497591553}, 'c': {1: 0.29713866415153034, 2: 0.09534677071786599, 3: 0.0021565477530057944, 4: 0.41679587768068516, 5: 0.002598935237702256, 6: 0.18596320445921039}, 'd': {1: 0.0890374213116738, 2: 0.49731733064548483, 3: 0.13226144047946445, 4: 0.0016710628743587734, 5: 4.080572809095719e-06, 6: 0.2797086641162089}} \n",
      "\n",
      "p_transition:  {'e': {'e': 0.0385012659014716, 'a': 3.571049522839872e-08, 'i': 0.07852227669132719, 'g': 0.08256553315483355, 'j': 0.08727770214590232, 'b': 0.08802939595643294, 'f': 0.0049401648077575955, 'h': 0.4205669501296641, 'c': 0.06127738955658445, 'd': 0.13689399802271424, None: 0.0014252879228166945}, 'a': {'e': 0.14673055143826147, 'a': 0.04666959569486886, 'i': 0.06450656550589241, 'g': 0.02083779884919397, 'j': 0.09462915140986505, 'b': 0.1215184881165398, 'f': 0.07826051354178502, 'h': 0.01802831790520332, 'c': 0.1946422311554781, 'd': 0.1739537118630613, None: 0.040223074519850674}, 'i': {'e': 0.029355715540366485, 'a': 0.10818795788440241, 'i': 0.01659738731559189, 'g': 0.018555979679922677, 'j': 0.0055265983625887105, 'b': 0.08200841587430416, 'f': 0.2965166568848692, 'h': 0.027481910355771976, 'c': 0.08458017313000216, 'd': 0.33104546642587007, None: 0.00014373854631008967}, 'g': {'e': 0.020195950380948312, 'a': 0.18013677336775955, 'i': 0.009745184548436113, 'g': 0.08107362073232416, 'j': 0.004339746065732216, 'b': 0.25888621105806603, 'f': 0.2031338778552105, 'h': 0.04982958938446069, 'c': 0.01400302556501665, 'd': 0.07113631320156598, None: 0.1075197078404799}, 'j': {'e': 0.00384586414882974, 'a': 0.05938647047770968, 'i': 0.0727798821010263, 'g': 0.005113795047021746, 'j': 0.3047972152557937, 'b': 0.24563220018681237, 'f': 0.01333248965627344, 'h': 0.019682190223744154, 'c': 0.07191771213341425, 'd': 0.1344900648082339, None: 0.0690221159611406}, 'b': {'e': 0.4300269985532702, 'a': 0.1796483497963386, 'i': 0.04261229290515476, 'g': 0.05442726800544614, 'j': 0.00635997276035456, 'b': 0.1010922155539355, 'f': 0.021297907301498886, 'h': 0.07407618452264916, 'c': 0.010939142909672921, 'd': 0.05947261627324443, None: 0.02004705141843444}, 'f': {'e': 0.07433856811702529, 'a': 0.48213423902968205, 'i': 0.06912819962140636, 'g': 0.0033512805875779137, 'j': 0.027409362114991054, 'b': 0.04803325064117813, 'f': 0.06940872899028785, 'h': 0.13018333696789516, 'c': 0.04138283739732032, 'd': 0.05399964702166395, None: 0.0006305495109719027}, 'h': {'e': 0.1506284297865123, 'a': 0.005241507445244362, 'i': 0.030400994306394397, 'g': 0.053092579123631425, 'j': 0.25128201437152625, 'b': 0.3767341494141965, 'f': 0.009513117693751592, 'h': 0.07148465255013041, 'c': 0.05084730663572765, 'd': 0.0007744293304003564, None: 8.193424846507555e-07}, 'c': {'e': 0.05472538472757748, 'a': 0.03384309947046661, 'i': 0.34658715701464665, 'g': 0.0030123843523401322, 'j': 0.12631620487306522, 'b': 0.12540545593346233, 'f': 0.044055371001243876, 'h': 0.046338334755308054, 'c': 0.17037967871314058, 'd': 0.03094758116741901, None: 0.018389347991329984}, 'd': {'e': 0.15265198136528751, 'a': 0.08811030823384379, 'i': 0.04865572256186396, 'g': 0.2420152159157843, 'j': 0.0036944017439528927, 'b': 0.03484464604837306, 'f': 0.06129021339562781, 'h': 0.01595540986543044, 'c': 0.10217933938184709, 'd': 0.221775007740881, None: 0.028827753747108102}, None: {'e': 0.004557269627391079, 'a': 0.0082620553823846, 'i': 0.0752080074253806, 'g': 0.24240226668805379, 'j': 0.006676142701941474, 'b': 0.11039822217810885, 'f': 0.03221668791855835, 'h': 0.0012143785749044475, 'c': 0.4928641153401916, 'd': 0.0013427144601934871, None: 0.024858139702891673}} \n",
      "\n",
      "auxiliary_transition_variables:  {'e': {'e': 1, 'a': 1, 'i': 1, 'g': 1, 'j': 1, 'b': 1, 'f': 1, 'h': 1, 'c': 1, 'd': 1}, 'a': {'e': 1, 'a': 1, 'i': 1, 'g': 1, 'j': 1, 'b': 1, 'f': 1, 'h': 1, 'c': 1, 'd': 1}, 'i': {'e': 1, 'a': 1, 'i': 1, 'g': 1, 'j': 1, 'b': 1, 'f': 1, 'h': 1, 'c': 1, 'd': 1}, 'g': {'e': 1, 'a': 1, 'i': 1, 'g': 1, 'j': 1, 'b': 1, 'f': 1, 'h': 1, 'c': 1, 'd': 1}, 'j': {'e': 1, 'a': 1, 'i': 1, 'g': 1, 'j': 1, 'b': 1, 'f': 1, 'h': 1, 'c': 1, 'd': 1}, 'b': {'e': 1, 'a': 1, 'i': 1, 'g': 1, 'j': 1, 'b': 1, 'f': 1, 'h': 1, 'c': 1, 'd': 1}, 'f': {'e': 1, 'a': 1, 'i': 1, 'g': 1, 'j': 1, 'b': 1, 'f': 1, 'h': 1, 'c': 1, 'd': 1}, 'h': {'e': 1, 'a': 1, 'i': 1, 'g': 1, 'j': 1, 'b': 1, 'f': 1, 'h': 1, 'c': 1, 'd': 1}, 'c': {'e': 1, 'a': 1, 'i': 1, 'g': 1, 'j': 1, 'b': 1, 'f': 1, 'h': 1, 'c': 1, 'd': 1}, 'd': {'e': 1, 'a': 1, 'i': 1, 'g': 1, 'j': 1, 'b': 1, 'f': 1, 'h': 1, 'c': 1, 'd': 1}} \n",
      "\n",
      "beta_transition:  {'e': 0.09066847978821883, 'a': 0.05092284765283178, 'i': 0.09103186983549148, 'g': 0.0821691456414217, 'j': 0.09373900012108698, 'b': 0.1508826265973153, 'f': 0.07446403763214945, 'h': 0.11673396850025759, 'c': 0.13206509745478662, 'd': 0.08928967806936977, None: 0.0280332487070706} \n",
      "\n",
      "beta_emission:  {1: 0.287356734293607, 2: 0.11037647047692731, 3: 0.21431402998738766, 4: 0.069340906482548, 5: 0.09385488426682699, 6: 0.22475697449270296} \n",
      "\n",
      "chain object:  [<bayesian_hmm.Chain, size 3>, <bayesian_hmm.Chain, size 3>] \n",
      "\n",
      "latent sequence:  ['h', 'b', 'e'] \n",
      "\n",
      "number of chains:  2 \n",
      "\n",
      "length of chains:  6 \n",
      "\n",
      "tabulate:  [['0' 'h' '1']\n",
      " ['0' 'b' '2']\n",
      " ['0' 'e' '3']\n",
      " ['1' 'f' '4']\n",
      " ['1' 'a' '5']\n",
      " ['1' 'j' '6']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"states: \", hmm.states, \"\\n\")\n",
    "\n",
    "print(\"n_initial \", hmm.n_initial, \"\\n\")\n",
    "\n",
    "print(\"n_transition: \", hmm.n_transition, \"\\n\")\n",
    "\n",
    "print(\"n_emission: \", hmm.n_emission, \"\\n\")\n",
    "\n",
    "print(\"hyperparams: \", hmm.hyperparameters, \"\\n\")\n",
    "\n",
    "print(\"p_initial: \", hmm.p_initial, \"\\n\")\n",
    "print(\"p_emission: \", hmm.p_emission, \"\\n\")\n",
    "print(\"p_transition: \", hmm.p_transition, \"\\n\")\n",
    "\n",
    "print(\"auxiliary_transition_variables: \", hmm.auxiliary_transition_variables, \"\\n\")\n",
    "print(\"beta_transition: \", hmm.beta_transition, \"\\n\")\n",
    "print(\"beta_emission: \", hmm.beta_emission, \"\\n\")\n",
    "\n",
    "print(\"chain object: \", hmm.chains, \"\\n\")\n",
    "\n",
    "print(\"latent sequence: \", hmm.chains[0].latent_sequence, \"\\n\")\n",
    "\n",
    "print(\"number of chains: \", hmm.c, \"\\n\")\n",
    "\n",
    "print(\"length of chains: \", hmm.n, \"\\n\")\n",
    "\n",
    "print(\"tabulate: \", hmm.tabulate(), \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g1'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(hmm._label_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.8538839655070956,\n",
       " 'gamma': 18.492683631680094,\n",
       " 'alpha_emission': 1.0899950209165887,\n",
       " 'gamma_emission': 20.0570355275628,\n",
       " 'kappa': 0}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{param: prior() for param, prior in hmm.priors.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.6811530574254632,\n",
       " 1.6811530574254632,\n",
       " 1.6811530574254632,\n",
       " 1.6811530574254632,\n",
       " 1.6811530574254632,\n",
       " 1.6811530574254632,\n",
       " 1.6811530574254632,\n",
       " 1.6811530574254632,\n",
       " 1.6811530574254632,\n",
       " 1.6811530574254632,\n",
       " 1.6811530574254632]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(np.random.dirichlet([18.492683631680094 / (10 + 1)] * (10 + 1)),reverse=True,)\n",
    "[18.492683631680094 / (10 + 1)] * (10 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n",
      "f\n"
     ]
    }
   ],
   "source": [
    "for chain in hmm.chains:\n",
    "    print(chain.latent_sequence[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import special\n",
    "#help(special.gamma)\n",
    "special.gamma(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'set' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bf416ab0f8af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# initialise chains\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchains\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# initialise hierarchical priors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/uchicago_courses/capstone/Bayesian-HMM/bayesian_hmm/chain.py\u001b[0m in \u001b[0;36minitialise\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m# create latent sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# update observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/random.py\u001b[0m in \u001b[0;36mchoices\u001b[0;34m(self, population, weights, cum_weights, k)\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0m_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0mcum_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_itertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/random.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0m_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0mcum_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_itertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'set' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "k = 20\n",
    "\"\"\"\n",
    "Initialise the HDPHMM. This involves:\n",
    "+ Choosing starting values for all hyperparameters\n",
    "+ Initialising all Chains (see Chain.initialise for further info)\n",
    "+ Initialising priors for probabilities (i.e. the Hierarchical priors)\n",
    "+ Updating all counts\n",
    "        \n",
    "sampling latent states, auxiliary beam variables,\n",
    "Typically called directly from a HDPHMM object.\n",
    ":param k: number of symbols to sample from for latent states\n",
    ":return: None\n",
    "\"\"\"\n",
    "    \n",
    "# create as many states as needed\n",
    "states = [next(_label_generator) for _ in range(k)]\n",
    "states = set(states)\n",
    "\n",
    "# set hyperparameters\n",
    "hyperparameters = {param: prior() for param, prior in priors.items()}\n",
    "\n",
    "# initialise chains\n",
    "for c in chains:\n",
    "    c.initialise(states)\n",
    "\n",
    "# initialise hierarchical priors\n",
    "temp_beta = sorted(\n",
    "    np.random.dirichlet(\n",
    "                [self.hyperparameters[\"gamma\"] / (self.k + 1)] * (self.k + 1)\n",
    "            ),\n",
    "            reverse=True,\n",
    "        )\n",
    "beta_transition = dict(zip(list(self.states) + [None], temp_beta))\n",
    "beta_transition = shrink_probabilities(beta_transition)\n",
    "auxiliary_transition_variables = {\n",
    "            s1: {s2: 1 for s2 in self.states.union({None})}\n",
    "            for s1 in self.states.union({None})\n",
    "        }\n",
    "\n",
    "# update counts before resampling\n",
    "_n_update()\n",
    "\n",
    "# resample remaining hyperparameters\n",
    "resample_beta_transition()\n",
    "resample_beta_emission()\n",
    "resample_p_initial()\n",
    "resample_p_transition()\n",
    "resample_p_emission()\n",
    "\n",
    "# set initialised flag\n",
    "_initialised = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'states' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-685bb5305ac3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"states: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gamma: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"gamma\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"n_initial: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_initial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'states' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"states: \", hmm.states)\n",
    "\n",
    "print(\"gamma: \", hyperparameters[\"gamma\"])\n",
    "\n",
    "print(\"n_initial: \", n_initial) # number of times a state is the initial state of the sequence\n",
    "print(\"n_transition: \", n_transition[label]) # \n",
    "print(\"n_emission: \", n_emission) # the number of times a particular emission belongs to each state\n",
    "\n",
    "print(\"p_initial: \",p_initial)\n",
    "print(\"p_emission: \", p_emission)\n",
    "print(\"p_transition: \",p_transition)\n",
    "\n",
    "print(\"auxiliary_transition_variables: \", auxiliary_transition_variables)\n",
    "print(\"beta_transition: \", beta_transition)\n",
    "print(\"beta_emission\", beta_emission)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-e1cda9f8b43d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# update emission probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m### don't understand the draws from dirichlet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtemp_p_emission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirichlet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"alpha\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbeta_emission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memissions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-156-e1cda9f8b43d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# update emission probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m### don't understand the draws from dirichlet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtemp_p_emission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirichlet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"alpha\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbeta_emission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memissions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "# update emission probabilities\n",
    "### don't understand the draws from dirichlet\n",
    "temp_p_emission = np.random.dirichlet([hyperparameters[\"alpha\"] * beta_emission[e] for e in emissions])\n",
    "\n",
    "\n",
    "### e.g.: array([1.])\n",
    "p_emission[label] = dict(zip(emissions, temp_p_emission))\n",
    "### e.g.: {None: {None: 1}, 'a1': {None: 0.9999999999999999}}\n",
    "        \n",
    "            \n",
    "# save label as a state \n",
    "states = states.union({label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{None: 1}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDPHMM(object):\n",
    "    \"\"\"\n",
    "    The Hierarchical Dirichlet Process Hidden Markov Model object. In fact, this is a\n",
    "    sticky-HDPHMM, since we allow a biased self-transition probability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        emission_sequences: Iterable[List[Optional[str]]],\n",
    "        emissions=None,  # type: ignore\n",
    "        # emissions: Optional[Iterable[Union[str, int]]] = None # ???\n",
    "        sticky: bool = True,\n",
    "        priors: Dict[str, Callable[[], Any]] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Create a Hierarchical Dirichlet Process Hidden Markov Model object, which can\n",
    "        (optionally) be sticky. The emission sequences must be provided, although all\n",
    "        other parameters are initialised with reasonable default values. It is\n",
    "        recommended to specify the `sticky` parameter, depending on whether you believe\n",
    "        the HMM to have a high probability of self-transition.\n",
    "        \n",
    "        :param emission_sequences: iterable, containing the observed emission sequences.\n",
    "        emission sequences can be different lengths, or zero length.\n",
    "        \n",
    "        :param emissions: set, optional. If not all emissions are guaranteed to be\n",
    "        observed in the data, this can be used to force non-zero emission probabilities\n",
    "        for unobserved emissions.\n",
    "        \n",
    "        :param sticky: bool, flag to indicate whether the HDPHMM is sticky or not.\n",
    "        Sticky HDPHMMs have an additional value (kappa) added to the probability of self\n",
    "        transition. It is recommended to set this depending on the knowledge of the\n",
    "        problem at hand.\n",
    "        \n",
    "        :param priors: dict, containing priors for the model hyperparameters. Priors\n",
    "        should be functions with zero arguments. The following priors are accepted:\n",
    "          + alpha: prior distribution of the alpha parameter. Alpha\n",
    "            parameter is the value used in the hierarchical Dirichlet prior for\n",
    "            transitions and starting probabilities. Higher values of alpha keep rows of\n",
    "            the transition matrix more similar to the beta parameters.\n",
    "          + gamma: prior distribution of the gamma parameter. Gamma controls the\n",
    "            strength of the uninformative prior in the starting and transition\n",
    "            distributions. Hence, it impacts the likelihood of resampling unseen states\n",
    "            when estimating beta coefficients. That is, higher values of gamma mean the\n",
    "            HMM is more likely to explore new states when resampling.\n",
    "          + alpha_emission: prior distribution of the alpha parameter for the\n",
    "            emission prior distribution. Alpha controls how tightly the conditional\n",
    "            emission distributions follow their hierarchical prior. Hence, higher values\n",
    "            of alpha_emission mean more strength in the hierarchical prior.\n",
    "          + gamma_emission: prior distribution of the gamma parameter for the\n",
    "            emission prior distribution. Gamma controls the strength of the\n",
    "            uninformative prior in the emission distribution. Hence, higher values of\n",
    "            gamma mean more strength of belief in the prior.\n",
    "          + kappa: prior distribution of the kappa parameter for the\n",
    "            self-transition probability. Ignored if `sticky==False`. Kappa prior should\n",
    "            have support in (0, 1) only. Higher values of kappa mean the chain is more\n",
    "            likely to explore states with high self-transition probabilty.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # store chains\n",
    "        self.chains = [Chain(sequence) for sequence in emission_sequences]\n",
    "        \n",
    "        # sticky flag\n",
    "        if type(sticky) is not bool:\n",
    "            raise ValueError(\"`sticky` must be type bool\")\n",
    "        self.sticky = sticky\n",
    "        \n",
    "        # store hyperparameter priors as callables\n",
    "        self.priors = {\n",
    "            \"alpha\": lambda: np.random.gamma(2, 2),\n",
    "            \"gamma\": lambda: np.random.gamma(3, 3),\n",
    "            \"alpha_emission\": lambda: np.random.gamma(2, 2),\n",
    "            \"gamma_emission\": lambda: np.random.gamma(3, 3),\n",
    "            \"kappa\": lambda: np.random.beta(1, 1),\n",
    "        }\n",
    "        # update prior params if given\n",
    "        if priors is not None:\n",
    "            self.priors.update(priors)\n",
    "        if len(self.priors) > 5:\n",
    "            raise ValueError(\"Unknown hyperparameter priors present\")\n",
    "            \n",
    "        # set kappa prior to one if not sticky    \n",
    "        if not self.sticky:\n",
    "            self.priors[\"kappa\"] = lambda: 0\n",
    "            if priors is not None and \"kappa\" in priors:\n",
    "                raise ValueError(\"`sticky` is False, but kappa prior function given\")\n",
    "                \n",
    "        # store initial hyperparameter values using existing callables\n",
    "        self.hyperparameters = {param: prior() for param, prior in self.priors.items()}\n",
    "        \n",
    "        # use internal properties to store fit hyperparameters\n",
    "\n",
    "        self.n_initial: InitDict\n",
    "        self.n_emission: NestedInitDict\n",
    "        self.n_transition: NestedInitDict\n",
    "        self.n_initial = {None: 0}\n",
    "        self.n_emission = {None: {None: 0}}\n",
    "        self.n_transition = {None: {None: 0}}\n",
    "        \n",
    "        # use internal properties to store current state for probabilities\n",
    "        self.p_initial: InitDict\n",
    "        self.p_emission: NestedInitDict\n",
    "        self.p_transition: NestedInitDict\n",
    "        self.p_initial = {None: 1}\n",
    "        self.p_emission = {None: {None: 1}}\n",
    "        self.p_transition = {None: {None: 1}}\n",
    "        \n",
    "        # store derived hyperparameters\n",
    "        self.auxiliary_transition_variables: NestedInitDict\n",
    "        self.beta_transition: InitDict\n",
    "        self.beta_emission: InitDict\n",
    "        self.auxiliary_transition_variables = {None: {None: 0}}\n",
    "        self.beta_transition = {None: 1}\n",
    "        self.beta_emission = {None: 1}\n",
    "        \n",
    "        # states & emissions\n",
    "        # TODO: figure out emissions's type...\n",
    "        if emissions is None:\n",
    "            emissions = functools.reduce(  # type: ignore\n",
    "                set.union, (set(c.emission_sequence) for c in self.chains), set()\n",
    "            )\n",
    "        elif not isinstance(emissions, set):\n",
    "            raise ValueError(\"emissions must be a set\")\n",
    "        self.emissions = emissions  # type: ignore\n",
    "        self.states: Set[Optional[str]] = set()\n",
    "\n",
    "        # generate non-repeating character labels for latent states\n",
    "        self._label_generator = label_generator(string.ascii_lowercase)\n",
    "\n",
    "        # keep flag to track initialisation\n",
    "        self._initialised = False\n",
    "        \n",
    "    @property\n",
    "    def initialised(self) -> bool:\n",
    "        \"\"\"\n",
    "        Test whether a HDPHMM is initialised.\n",
    "        :return: bool\n",
    "        \"\"\"\n",
    "        return self._initialised\n",
    "    \n",
    "    @initialised.setter\n",
    "    def initialised(self, value: Any) -> None:\n",
    "        if value:\n",
    "            raise AssertionError(\"HDPHMM must be initialised through initialise method\")\n",
    "        elif not value:\n",
    "            self._initialised = False\n",
    "        else:\n",
    "            raise ValueError(\"initialised flag must be Boolean\")\n",
    "            \n",
    "    @property\n",
    "    def c(self) -> int:\n",
    "        \"\"\"\n",
    "        Number of chains in the HMM.\n",
    "        :return: int\n",
    "        \"\"\"\n",
    "        return len(self.chains)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def k(self) -> int:\n",
    "        \"\"\"\n",
    "        Number of latent states in the HMM currently.\n",
    "        :return: int\n",
    "        \"\"\"\n",
    "        return len(self.states)\n",
    "    \n",
    "    @property\n",
    "    def n(self) -> int:\n",
    "        \"\"\"\n",
    "        Number of unique emissions. If `emissions` was specified when the HDPHMM was\n",
    "        created, then this counts the number of elements in `emissions`. Otherwise,\n",
    "        counts the number of observed emissions across all emission sequences.\n",
    "        :return: int\n",
    "        \"\"\"\n",
    "        return len(self.emissions)\n",
    "    \n",
    "    \n",
    "    def tabulate(self) -> np.array:\n",
    "        \"\"\"\n",
    "        Convert the latent and emission sequences for all chains into a single numpy\n",
    "        array. Array contains an index which matches a Chain's index in\n",
    "        HDPHMM.chains, the current latent state, and the emission for all chains at\n",
    "        all times.\n",
    "        :return: numpy array with dimension (l, 3), where l is the length of the Chain\n",
    "        \"\"\"\n",
    "        hmm_array = np.concatenate(\n",
    "            tuple(\n",
    "                np.concatenate(\n",
    "                    (np.array([[n] * self.chains[n].T]).T, self.chains[n].tabulate()),\n",
    "                    axis=1,\n",
    "                )\n",
    "                for n in range(self.c)\n",
    "            ),\n",
    "            axis=0,\n",
    "        )\n",
    "        return hmm_array\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return \"<bayesian_hmm.HDPHMM, size {C}>\".format(C=self.c)\n",
    "    \n",
    "    def __str__(self, print_len: int = 15) -> str:\n",
    "        fs = (\n",
    "            \"bayesian_hmm.HDPHMM,\"\n",
    "            + \" ({C} chains, {K} states, {N} emissions, {Ob} observations)\"\n",
    "        )\n",
    "        return fs.format(C=self.c, K=self.k, N=self.n, Ob=sum(c.T for c in self.chains))\n",
    "    \n",
    "    \n",
    "    def state_generator(self, eps: Numeric = 1e-12) -> Generator[str, None, None]:\n",
    "        \"\"\"\n",
    "        Create a new state for the HDPHMM, and update all parameters accordingly.\n",
    "        This involves updating\n",
    "          + The counts for the new symbol\n",
    "          + The auxiliary variables for the new symbol\n",
    "          + The probabilities for the new symbol\n",
    "          + The states captured by the HDPHMM\n",
    "        :return: str, label of the new state\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            \n",
    "            \"\"\"\n",
    "            self.'n' s\n",
    "            \"\"\"\n",
    "            \n",
    "            label = next(self._label_generator) # generate label for state name\n",
    "\n",
    "            # update counts with zeros (assume _n_update called later)\n",
    "            # state irrelevant for constant count (all zeros)\n",
    "            self.n_initial[label] = 0 # set n_initial value corresponding to label to zero\n",
    "            self.n_transition[label] = {s: 0 for s in self.states.union({label, None})} # n_transitions for new state\n",
    "            for s in self.states: # could probably use same format as line above\n",
    "                self.n_transition[s].update({label: 0}) # n_transition for new state to 0\n",
    "            self.n_emission[label] = {e: 0 for e in self.emissions} # n_emissions for new state to 0\n",
    "\n",
    "            # update auxiliary transition variables\n",
    "            self.auxiliary_transition_variables[label] = { #### don't quite undertand aux transition variables\n",
    "                s2: 1 for s2 in list(self.states) + [label, None]\n",
    "            }\n",
    "            for s1 in self.states:\n",
    "                self.auxiliary_transition_variables[s1][label] = 1\n",
    "            \"\"\"\n",
    "            self.beta_transition\n",
    "            \"\"\"\n",
    "            # update beta_transition value and split out from current pseudo state\n",
    "            temp_beta = np.random.beta(1, self.hyperparameters[\"gamma\"]) # use prior gamma to generate value from beta\n",
    "            self.beta_transition[label] = temp_beta * self.beta_transition[None] # Stick breaking: None is dict key\n",
    "            self.beta_transition[None] = (1 - temp_beta) * self.beta_transition[None] # update value for None key using 1-p\n",
    "            \n",
    "            \"\"\"\n",
    "            self.p_initial\n",
    "            \"\"\"\n",
    "            # update starting probability \n",
    "            ### same thing as before\n",
    "            temp_p_initial = np.random.beta(1, self.hyperparameters[\"gamma\"])  # use prior gamma to generate value from beta\n",
    "            self.p_initial[label] = temp_p_initial * self.p_initial[None] # stick breaking: break up temp_p_initial\n",
    "            self.p_initial[None] = (1 - temp_p_initial) * self.p_initial[None] # and then isolate the rest\n",
    "            \n",
    "            \"\"\"\n",
    "            self.p_transition\n",
    "            \"\"\"\n",
    "            \n",
    "            # update transition from new state\n",
    "            ### draw from dirichlet dist using betas generated from stick breaking\n",
    "            temp_p_transition = np.random.dirichlet([self.beta_transition[s] for s in list(self.states) + [label, None]])\n",
    "            p_transition_label = dict(zip(list(self.states) + [label, None], temp_p_transition)) # e.g.: {'a1': 0.9781489319558994, None: 0.021851068044100645}\n",
    "            self.p_transition[label] = shrink_probabilities(p_transition_label, eps) ### shrink resulting probs ever so slightly so that they don't quite sum up to 1\n",
    "                                    ### e.g. {None: {None: 1}, 'a1': {'a1': 0.9781489319549431, None: 0.021851068045056942}}\n",
    "                \n",
    "                \n",
    "                \n",
    "            # update transitions into new state\n",
    "            for state in self.states.union({None}): # (note that label not included in self.states)\n",
    "                \n",
    "                \n",
    "                ### more stick breaking\n",
    "                temp_p_transition = np.random.beta(1, self.hyperparameters[\"gamma\"])\n",
    "                self.p_transition[state][label] = (self.p_transition[state][None] * temp_p_transition)\n",
    "                self.p_transition[state][None] = self.p_transition[state][None] * (1 - temp_p_transition)\n",
    "\n",
    "                ### {None: {None: 0.07818045843222077, 'a1': 0.9218195415677792}, 'a1': {'a1': 0.9781489319549431, None: 0.021851068045056942}}\n",
    "                \n",
    "            # update emission probabilities\n",
    "            ### don't understand the draws from dirichlet\n",
    "            temp_p_emission = np.random.dirichlet([self.hyperparameters[\"alpha\"] * self.beta_emission[e] for e in self.emissions])\n",
    "                ### e.g.: array([1.])\n",
    "            self.p_emission[label] = dict(zip(self.emissions, temp_p_emission))\n",
    "                ### e.g.: {None: {None: 1}, 'a1': {None: 0.9999999999999999}}\n",
    "        \n",
    "            \n",
    "            # save label as a state \n",
    "            self.states = self.states.union({label})\n",
    "\n",
    "            yield label\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_sequence = [[7,6,3,53,45,8,75,109],[7,45,1,8,7,6,2,67]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = HDPHMM(emission_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.HDPHMM.__init__.<locals>.<lambda>()>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdf.priors['alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "zxcv = lambda: np.random.gamma(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{None, 'thing'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.union({\"thing\",None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = {\n",
    "            \"alpha\": lambda: np.random.gamma(2, 2),\n",
    "            \"gamma\": lambda: np.random.gamma(3, 3),\n",
    "            \"alpha_emission\": lambda: np.random.gamma(2, 2),\n",
    "            \"gamma_emission\": lambda: np.random.gamma(3, 3),\n",
    "            \"kappa\": lambda: np.random.beta(1, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': <function __main__.<lambda>()>,\n",
       " 'gamma': <function __main__.<lambda>()>,\n",
       " 'alpha_emission': <function __main__.<lambda>()>,\n",
       " 'gamma_emission': <function __main__.<lambda>()>,\n",
       " 'kappa': <function __main__.<lambda>()>}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1.0819281624774297,\n",
       " 'gamma': 2.1454465997108616,\n",
       " 'alpha_emission': 1.681501928063262,\n",
       " 'gamma_emission': 4.761217109850887,\n",
       " 'kappa': 0.13788877309004735}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{param: prior() for param, prior in priors.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Hierarchical Dirichlet Process Hidden Markov Model (HDPHMM).\n",
    "The HDPHMM object collects a number of observed emission sequences, and estimates\n",
    "latent states at every time point, along with a probability structure that ties latent\n",
    "states to emissions. This structure involves\n",
    "  + A starting probability, which dictates the probability that the first state\n",
    "  in a latent seqeuence is equal to a given symbol. This has a hierarchical Dirichlet\n",
    "  prior.\n",
    "  + A transition probability, which dictates the probability that any given symbol in\n",
    "  the latent sequence is followed by another given symbol. This shares the same\n",
    "  hierarchical Dirichlet prior as the starting probabilities.\n",
    "  + An emission probability, which dictates the probability that any given emission\n",
    "  is observed conditional on the latent state at the same time point. This uses a\n",
    "  Dirichlet prior.\n",
    "Fitting HDPHMMs requires MCMC estimation. MCMC estimation is thus used to calculate the\n",
    "posterior distribution for the above probabilities. In addition, we can use MAP\n",
    "estimation (for example) to fix latent states, and facilitate further analysis of a\n",
    "Chain.\n",
    "\"\"\"\n",
    "# Support typehinting.\n",
    "from __future__ import annotations\n",
    "from typing import Any, Union, Optional, Set, Dict, Iterable, List, Callable, Generator\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import terminaltables\n",
    "import tqdm\n",
    "import functools\n",
    "import multiprocessing\n",
    "import string\n",
    "from scipy import special, stats\n",
    "from sympy.functions.combinatorial.numbers import stirling\n",
    "from chain import Chain\n",
    "from utils import label_generator, dirichlet_process_generator, shrink_probabilities\n",
    "from warnings import catch_warnings\n",
    "\n",
    "# Shorthand for numeric types.\n",
    "Numeric = Union[int, float]\n",
    "\n",
    "# Oft-used dictionary initializations with shorthands.\n",
    "DictStrNum = Dict[Optional[str], Numeric]\n",
    "InitDict = DictStrNum\n",
    "DictStrDictStrNum = Dict[Optional[str], DictStrNum]\n",
    "NestedInitDict = DictStrDictStrNum\n",
    "\n",
    "\n",
    "class HDPHMM(object):\n",
    "    \"\"\"\n",
    "    The Hierarchical Dirichlet Process Hidden Markov Model object. In fact, this is a\n",
    "    sticky-HDPHMM, since we allow a biased self-transition probability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        emission_sequences: Iterable[List[Optional[str]]],\n",
    "        emissions=None,  # type: ignore\n",
    "        # emissions: Optional[Iterable[Union[str, int]]] = None # ???\n",
    "        sticky: bool = True,\n",
    "        priors: Dict[str, Callable[[], Any]] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Create a Hierarchical Dirichlet Process Hidden Markov Model object, which can\n",
    "        (optionally) be sticky. The emission sequences must be provided, although all\n",
    "        other parameters are initialised with reasonable default values. It is\n",
    "        recommended to specify the `sticky` parameter, depending on whether you believe\n",
    "        the HMM to have a high probability of self-transition.\n",
    "        \n",
    "        :param emission_sequences: iterable, containing the observed emission sequences.\n",
    "        emission sequences can be different lengths, or zero length.\n",
    "        \n",
    "        :param emissions: set, optional. If not all emissions are guaranteed to be\n",
    "        observed in the data, this can be used to force non-zero emission probabilities\n",
    "        for unobserved emissions.\n",
    "        \n",
    "        :param sticky: bool, flag to indicate whether the HDPHMM is sticky or not.\n",
    "        Sticky HDPHMMs have an additional value (kappa) added to the probability of self\n",
    "        transition. It is recommended to set this depending on the knowledge of the\n",
    "        problem at hand.\n",
    "        \n",
    "        :param priors: dict, containing priors for the model hyperparameters. Priors\n",
    "        should be functions with zero arguments. The following priors are accepted:\n",
    "          + alpha: prior distribution of the alpha parameter. Alpha\n",
    "            parameter is the value used in the hierarchical Dirichlet prior for\n",
    "            transitions and starting probabilities. Higher values of alpha keep rows of\n",
    "            the transition matrix more similar to the beta parameters.\n",
    "          + gamma: prior distribution of the gamma parameter. Gamma controls the\n",
    "            strength of the uninformative prior in the starting and transition\n",
    "            distributions. Hence, it impacts the likelihood of resampling unseen states\n",
    "            when estimating beta coefficients. That is, higher values of gamma mean the\n",
    "            HMM is more likely to explore new states when resampling.\n",
    "          + alpha_emission: prior distribution of the alpha parameter for the\n",
    "            emission prior distribution. Alpha controls how tightly the conditional\n",
    "            emission distributions follow their hierarchical prior. Hence, higher values\n",
    "            of alpha_emission mean more strength in the hierarchical prior.\n",
    "          + gamma_emission: prior distribution of the gamma parameter for the\n",
    "            emission prior distribution. Gamma controls the strength of the\n",
    "            uninformative prior in the emission distribution. Hence, higher values of\n",
    "            gamma mean more strength of belief in the prior.\n",
    "          + kappa: prior distribution of the kappa parameter for the\n",
    "            self-transition probability. Ignored if `sticky==False`. Kappa prior should\n",
    "            have support in (0, 1) only. Higher values of kappa mean the chain is more\n",
    "            likely to explore states with high self-transition probabilty.\n",
    "        \"\"\"\n",
    "        # store chains\n",
    "        self.chains = [Chain(sequence) for sequence in emission_sequences]\n",
    "\n",
    "        # sticky flag\n",
    "        if type(sticky) is not bool:\n",
    "            raise ValueError(\"`sticky` must be type bool\")\n",
    "        self.sticky = sticky\n",
    "\n",
    "        # store hyperparameter priors\n",
    "        self.priors = {\n",
    "            \"alpha\": lambda: np.random.gamma(2, 2),\n",
    "            \"gamma\": lambda: np.random.gamma(3, 3),\n",
    "            \"alpha_emission\": lambda: np.random.gamma(2, 2),\n",
    "            \"gamma_emission\": lambda: np.random.gamma(3, 3),\n",
    "            \"kappa\": lambda: np.random.beta(1, 1),\n",
    "        }\n",
    "        if priors is not None:\n",
    "            self.priors.update(priors)\n",
    "        if len(self.priors) > 5:\n",
    "            raise ValueError(\"Unknown hyperparameter priors present\")\n",
    "\n",
    "        if not self.sticky:\n",
    "            self.priors[\"kappa\"] = lambda: 0\n",
    "            if priors is not None and \"kappa\" in priors:\n",
    "                raise ValueError(\"`sticky` is False, but kappa prior function given\")\n",
    "\n",
    "        # store initial hyperparameter values\n",
    "        self.hyperparameters = {param: prior() for param, prior in self.priors.items()}\n",
    "\n",
    "        # use internal properties to store fit hyperparameters\n",
    "        self.n_initial: InitDict\n",
    "        self.n_emission: NestedInitDict\n",
    "        self.n_transition: NestedInitDict\n",
    "        self.n_initial = {None: 0}\n",
    "        self.n_emission = {None: {None: 0}}\n",
    "        self.n_transition = {None: {None: 0}}\n",
    "\n",
    "        # use internal properties to store current state for probabilities\n",
    "        self.p_initial: InitDict\n",
    "        self.p_emission: NestedInitDict\n",
    "        self.p_transition: NestedInitDict\n",
    "        self.p_initial = {None: 1}\n",
    "        self.p_emission = {None: {None: 1}}\n",
    "        self.p_transition = {None: {None: 1}}\n",
    "\n",
    "        # store derived hyperparameters\n",
    "        self.auxiliary_transition_variables: NestedInitDict\n",
    "        self.beta_transition: InitDict\n",
    "        self.beta_emission: InitDict\n",
    "        self.auxiliary_transition_variables = {None: {None: 0}}\n",
    "        self.beta_transition = {None: 1}\n",
    "        self.beta_emission = {None: 1}\n",
    "\n",
    "        # states & emissions\n",
    "        # TODO: figure out emissions's type...\n",
    "        if emissions is None:\n",
    "            emissions = functools.reduce(  # type: ignore\n",
    "                set.union, (set(c.emission_sequence) for c in self.chains), set()\n",
    "            )\n",
    "        elif not isinstance(emissions, set):\n",
    "            raise ValueError(\"emissions must be a set\")\n",
    "        self.emissions = emissions  # type: ignore\n",
    "        self.states: Set[Optional[str]] = set()\n",
    "\n",
    "        # generate non-repeating character labels for latent states\n",
    "        self._label_generator = label_generator(string.ascii_lowercase)\n",
    "\n",
    "        # keep flag to track initialisation\n",
    "        self._initialised = False\n",
    "\n",
    "    @property\n",
    "    def initialised(self) -> bool:\n",
    "        \"\"\"\n",
    "        Test whether a HDPHMM is initialised.\n",
    "        :return: bool\n",
    "        \"\"\"\n",
    "        return self._initialised\n",
    "\n",
    "    @initialised.setter\n",
    "    def initialised(self, value: Any) -> None:\n",
    "        if value:\n",
    "            raise AssertionError(\"HDPHMM must be initialised through initialise method\")\n",
    "        elif not value:\n",
    "            self._initialised = False\n",
    "        else:\n",
    "            raise ValueError(\"initialised flag must be Boolean\")\n",
    "\n",
    "    @property\n",
    "    def c(self) -> int:\n",
    "        \"\"\"\n",
    "        Number of chains in the HMM.\n",
    "        :return: int\n",
    "        \"\"\"\n",
    "        return len(self.chains)\n",
    "\n",
    "    @property\n",
    "    def k(self) -> int:\n",
    "        \"\"\"\n",
    "        Number of latent states in the HMM currently.\n",
    "        :return: int\n",
    "        \"\"\"\n",
    "        return len(self.states)\n",
    "\n",
    "    @property\n",
    "    def n(self) -> int:\n",
    "        \"\"\"\n",
    "        Number of unique emissions. If `emissions` was specified when the HDPHMM was\n",
    "        created, then this counts the number of elements in `emissions`. Otherwise,\n",
    "        counts the number of observed emissions across all emission sequences.\n",
    "        :return: int\n",
    "        \"\"\"\n",
    "        return len(self.emissions)\n",
    "\n",
    "    def tabulate(self) -> np.array:\n",
    "        \"\"\"\n",
    "        Convert the latent and emission sequences for all chains into a single numpy\n",
    "        array. Array contains an index which matches a Chain's index in\n",
    "        HDPHMM.chains, the current latent state, and the emission for all chains at\n",
    "        all times.\n",
    "        :return: numpy array with dimension (l, 3), where l is the length of the Chain\n",
    "        \"\"\"\n",
    "        hmm_array = np.concatenate(\n",
    "            tuple(\n",
    "                np.concatenate(\n",
    "                    (np.array([[n] * self.chains[n].T]).T, self.chains[n].tabulate()),\n",
    "                    axis=1,\n",
    "                )\n",
    "                for n in range(self.c)\n",
    "            ),\n",
    "            axis=0,\n",
    "        )\n",
    "        return hmm_array\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"<bayesian_hmm.HDPHMM, size {C}>\".format(C=self.c)\n",
    "\n",
    "    def __str__(self, print_len: int = 15) -> str:\n",
    "        fs = (\n",
    "            \"bayesian_hmm.HDPHMM,\"\n",
    "            + \" ({C} chains, {K} states, {N} emissions, {Ob} observations)\"\n",
    "        )\n",
    "        return fs.format(C=self.c, K=self.k, N=self.n, Ob=sum(c.T for c in self.chains))\n",
    "\n",
    "    def state_generator(self, eps: Numeric = 1e-12) -> Generator[str, None, None]:\n",
    "        \"\"\"\n",
    "        Create a new state for the HDPHMM, and update all parameters accordingly.\n",
    "        This involves updating\n",
    "          + The counts for the new symbol\n",
    "          + The auxiliary variables for the new symbol\n",
    "          + The probabilities for the new symbol\n",
    "          + The states captured by the HDPHMM\n",
    "        :return: str, label of the new state\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            label = next(self._label_generator)\n",
    "\n",
    "            # update counts with zeros (assume _n_update called later)\n",
    "            # state irrelevant for constant count (all zeros)\n",
    "            self.n_initial[label] = 0\n",
    "            self.n_transition[label] = {s: 0 for s in self.states.union({label, None})}\n",
    "            for s in self.states:\n",
    "                self.n_transition[s].update({label: 0})\n",
    "            self.n_emission[label] = {e: 0 for e in self.emissions}\n",
    "\n",
    "            # update auxiliary transition variables\n",
    "            self.auxiliary_transition_variables[label] = {\n",
    "                s2: 1 for s2 in list(self.states) + [label, None]\n",
    "            }\n",
    "            for s1 in self.states:\n",
    "                self.auxiliary_transition_variables[s1][label] = 1\n",
    "\n",
    "            # update beta_transition value and split out from current pseudo state\n",
    "            temp_beta = np.random.beta(1, self.hyperparameters[\"gamma\"])\n",
    "            self.beta_transition[label] = temp_beta * self.beta_transition[None]\n",
    "            self.beta_transition[None] = (1 - temp_beta) * self.beta_transition[None]\n",
    "\n",
    "            # update starting probability\n",
    "            temp_p_initial = np.random.beta(1, self.hyperparameters[\"gamma\"])\n",
    "            self.p_initial[label] = temp_p_initial * self.p_initial[None]\n",
    "            self.p_initial[None] = (1 - temp_p_initial) * self.p_initial[None]\n",
    "\n",
    "            # update transition from new state\n",
    "            temp_p_transition = np.random.dirichlet(\n",
    "                [self.beta_transition[s] for s in list(self.states) + [label, None]]\n",
    "            )\n",
    "            p_transition_label = dict(\n",
    "                zip(list(self.states) + [label, None], temp_p_transition)\n",
    "            )\n",
    "            self.p_transition[label] = shrink_probabilities(p_transition_label, eps)\n",
    "\n",
    "            # update transitions into new state\n",
    "            for state in self.states.union({None}):\n",
    "                # (note that label not included in self.states)\n",
    "                temp_p_transition = np.random.beta(1, self.hyperparameters[\"gamma\"])\n",
    "                self.p_transition[state][label] = (\n",
    "                    self.p_transition[state][None] * temp_p_transition\n",
    "                )\n",
    "                self.p_transition[state][None] = self.p_transition[state][None] * (\n",
    "                    1 - temp_p_transition\n",
    "                )\n",
    "\n",
    "            # update emission probabilities\n",
    "            temp_p_emission = np.random.dirichlet(\n",
    "                [\n",
    "                    self.hyperparameters[\"alpha\"] * self.beta_emission[e] for e in self.emissions\n",
    "                ]\n",
    "            )\n",
    "            self.p_emission[label] = dict(zip(self.emissions, temp_p_emission))\n",
    "\n",
    "            # save label\n",
    "            self.states = self.states.union({label})\n",
    "\n",
    "            yield label\n",
    "\n",
    "    def initialise(self, k: int = 20) -> None:\n",
    "        \"\"\"\n",
    "        Initialise the HDPHMM. This involves:\n",
    "          + Choosing starting values for all hyperparameters\n",
    "          + Initialising all Chains (see Chain.initialise for further info)\n",
    "          + Initialising priors for probabilities (i.e. the Hierarchical priors)\n",
    "          + Updating all counts\n",
    "        \n",
    "        sampling latent states, auxiliary beam variables,\n",
    "        Typically called directly from a HDPHMM object.\n",
    "        :param k: number of symbols to sample from for latent states\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # create as many states as needed\n",
    "        states = [next(self._label_generator) for _ in range(k)]\n",
    "        self.states = set(states)\n",
    "\n",
    "        # set hyperparameters\n",
    "        self.hyperparameters = {param: prior() for param, prior in self.priors.items()}\n",
    "\n",
    "        # initialise chains\n",
    "        for c in self.chains:\n",
    "            c.initialise(states)\n",
    "\n",
    "        # initialise hierarchical priors\n",
    "        temp_beta = sorted(\n",
    "            np.random.dirichlet(\n",
    "                [self.hyperparameters[\"gamma\"] / (self.k + 1)] * (self.k + 1)\n",
    "            ),\n",
    "            reverse=True,\n",
    "        )\n",
    "        beta_transition = dict(zip(list(self.states) + [None], temp_beta))\n",
    "        self.beta_transition = shrink_probabilities(beta_transition)\n",
    "        self.auxiliary_transition_variables = {\n",
    "            s1: {s2: 1 for s2 in self.states.union({None})}\n",
    "            for s1 in self.states.union({None})\n",
    "        }\n",
    "\n",
    "        # update counts before resampling\n",
    "        self._n_update()\n",
    "\n",
    "        # resample remaining hyperparameters\n",
    "        self.resample_beta_transition()\n",
    "        self.resample_beta_emission()\n",
    "        self.resample_p_initial()\n",
    "        self.resample_p_transition()\n",
    "        self.resample_p_emission()\n",
    "\n",
    "        # set initialised flag\n",
    "        self._initialised = True\n",
    "\n",
    "    def update_states(self):\n",
    "        \"\"\"\n",
    "        Remove defunct states from the internal set of states, and merge all parameters\n",
    "        associated with these states back into the 'None' values.\n",
    "        \"\"\"\n",
    "        # identify states to merge\n",
    "        states_prev = self.states\n",
    "        states_next = set(\n",
    "            sorted(\n",
    "                functools.reduce(\n",
    "                    set.union, (set(c.latent_sequence) for c in self.chains), set()\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        states_removed = states_prev - states_next\n",
    "\n",
    "        # merge old probabilities into None\n",
    "        for state in states_removed:\n",
    "            # remove entries and add to aggregate None state\n",
    "            self.beta_transition[None] += self.beta_transition.pop(state)\n",
    "            self.p_initial[None] += self.p_initial.pop(state)\n",
    "            for s1 in states_next.union({None}):\n",
    "                self.p_transition[s1][None] += self.p_transition[s1].pop(state)\n",
    "\n",
    "            # remove transition vector entirely\n",
    "            del self.p_transition[state]\n",
    "\n",
    "        # update internal state tracking\n",
    "        self.states = states_next\n",
    "\n",
    "    def _n_update(self):\n",
    "        \"\"\"\n",
    "        Update counts required for resampling probabilities. These counts are used\n",
    "        to sample from the posterior distribution for probabilities. This function\n",
    "        should be called after any latent state is changed, including after resampling.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # check that all chains are initialised\n",
    "        if any(not chain.initialised_flag for chain in self.chains):\n",
    "            raise AssertionError(\n",
    "                \"Chains must be initialised before calculating fit parameters\"\n",
    "            )\n",
    "\n",
    "        # transition count for non-oracle transitions\n",
    "        n_initial = {s: 0 for s in self.states.union({None})}\n",
    "        n_emission = {\n",
    "            s: {e: 0 for e in self.emissions} for s in self.states.union({None})\n",
    "        }\n",
    "        n_transition = {\n",
    "            s1: {s2: 0 for s2 in self.states.union({None})}\n",
    "            for s1 in self.states.union({None})\n",
    "        }\n",
    "\n",
    "        # increment all relevant hyperparameters while looping over sequence\n",
    "        for chain in self.chains:\n",
    "            # increment initial states emitted by oracle\n",
    "            n_initial[chain.latent_sequence[0]] += 1\n",
    "\n",
    "            # increment emissions only for final state\n",
    "            n_emission[chain.latent_sequence[chain.T - 1]][\n",
    "                chain.emission_sequence[chain.T - 1]\n",
    "            ] += 1\n",
    "\n",
    "            # increment all transitions and emissions within chain\n",
    "            for t in range(chain.T - 1):\n",
    "                n_emission[chain.latent_sequence[t]][chain.emission_sequence[t]] += 1\n",
    "                n_transition[chain.latent_sequence[t]][\n",
    "                    chain.latent_sequence[t + 1]\n",
    "                ] += 1\n",
    "\n",
    "        # store recalculated fit hyperparameters\n",
    "        self.n_initial = n_initial\n",
    "        self.n_emission = n_emission\n",
    "        self.n_transition = n_transition\n",
    "\n",
    "    @staticmethod\n",
    "    def _resample_auxiliary_transition_atom_complete(\n",
    "        alpha, beta, n, use_approximation=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Use a resampling approach that estimates probabilities for all auxiliary\n",
    "        transition parameters. This avoids the slowdown in convergence caused by\n",
    "        Metropolis Hastings rejections, but is more computationally costly.\n",
    "        :param alpha:\n",
    "        :param beta:\n",
    "        :param n:\n",
    "        :param use_approximation:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # initialise values required to resample\n",
    "        p_required = np.random.uniform(0, 1)\n",
    "        m = 0\n",
    "        p_cumulative = 0\n",
    "        scale = alpha * beta\n",
    "\n",
    "        if not use_approximation:\n",
    "            # use precise probabilities\n",
    "            try:\n",
    "                logp_constant = np.log(special.gamma(scale)) - np.log(\n",
    "                    special.gamma(scale + n)\n",
    "                )\n",
    "                while p_cumulative == 0 or p_cumulative < p_required and m < n:\n",
    "                    # accumulate probability\n",
    "                    m += 1\n",
    "                    logp_accept = (\n",
    "                        m * np.log(scale)\n",
    "                        + np.log(stirling(n, m, kind=1))\n",
    "                        + logp_constant\n",
    "                    )\n",
    "                    p_cumulative += np.exp(logp_accept)\n",
    "            # after one failure use only the approximation\n",
    "            except (RecursionError, OverflowError):\n",
    "                # correct for failed case before\n",
    "                m -= 1\n",
    "        while p_cumulative < p_required and m < n:\n",
    "            # problems with stirling recursion (large n & m), use approximation instead\n",
    "            # magic number is the Euler constant\n",
    "            # approximation derived in documentation\n",
    "            m += 1\n",
    "            logp_accept = (\n",
    "                m\n",
    "                + (m + scale - 0.5) * np.log(scale)\n",
    "                + (m - 1) * np.log(0.57721 + np.log(n - 1))\n",
    "                - (m - 0.5) * np.log(m)\n",
    "                - scale * np.log(scale + n)\n",
    "                - scale\n",
    "            )\n",
    "            p_cumulative += np.exp(logp_accept)\n",
    "        # breaks out of loop after m is sufficiently large\n",
    "        return max(m, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _resample_auxiliary_transition_atom_mh(\n",
    "        alpha, beta, n, m_curr, use_approximation=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Use a Metropolos Hastings resampling approach that often rejects the proposed\n",
    "        value. This can cause the convergence to slow down (as the values are less\n",
    "        dynamic) but speeds up the computation.\n",
    "        :param alpha:\n",
    "        :param beta:\n",
    "        :param n:\n",
    "        :param m_curr:\n",
    "        :param use_approximation:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # propose new m\n",
    "        n = max(n, 1)\n",
    "        m_proposed = random.choice(range(1, n + 1))\n",
    "        if m_curr > n:\n",
    "            return m_proposed\n",
    "\n",
    "        # find relative probabilities\n",
    "        if use_approximation and n > 10:\n",
    "            logp_diff = (\n",
    "                (m_proposed - 0.5) * np.log(m_proposed)\n",
    "                - (m_curr - 0.5) * np.log(m_curr)\n",
    "                + (m_proposed - m_curr) * np.log(alpha * beta * np.exp(1))\n",
    "                + (m_proposed - m_curr) * np.log(0.57721 + np.log(n - 1))\n",
    "            )\n",
    "        else:\n",
    "            p_curr = float(stirling(n, m_curr, kind=1)) * ((alpha * beta) ** m_curr)\n",
    "            p_proposed = float(stirling(n, m_proposed, kind=1)) * (\n",
    "                (alpha * beta) ** m_proposed\n",
    "            )\n",
    "            logp_diff = np.log(p_proposed) - np.log(p_curr)\n",
    "\n",
    "        # use MH variable to decide whether to accept m_proposed\n",
    "        with catch_warnings(record=True) as caught_warnings:\n",
    "            p_accept = min(1, np.exp(logp_diff))\n",
    "            p_accept = bool(np.random.binomial(n=1, p=p_accept))  # convert to boolean\n",
    "            if caught_warnings:\n",
    "                p_accept = True\n",
    "\n",
    "        return m_proposed if p_accept else m_curr\n",
    "\n",
    "    @staticmethod\n",
    "    def _resample_auxiliary_transition_atom(\n",
    "        state_pair,\n",
    "        alpha,\n",
    "        beta,\n",
    "        n_initial,\n",
    "        n_transition,\n",
    "        auxiliary_transition_variables,\n",
    "        resample_type=\"mh\",\n",
    "        use_approximation=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Resampling the auxiliary transition atoms should be performed before resampling\n",
    "        the transition beta values. This is the static method, creating to allow for\n",
    "        parallelised resampling.\n",
    "        :param state_pair:\n",
    "        :param alpha:\n",
    "        :param beta:\n",
    "        :param n_initial:\n",
    "        :param n_transition:\n",
    "        :param auxiliary_transition_variables:\n",
    "        :param resample_type:\n",
    "        :param use_approximation:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # extract states\n",
    "        state1, state2 = state_pair\n",
    "\n",
    "        # apply resampling\n",
    "        if resample_type == \"mh\":\n",
    "            return HDPHMM._resample_auxiliary_transition_atom_mh(\n",
    "                alpha,\n",
    "                beta[state2],\n",
    "                n_initial[state2] + n_transition[state1][state2],\n",
    "                auxiliary_transition_variables[state1][state2],\n",
    "                use_approximation,\n",
    "            )\n",
    "        elif resample_type == \"complete\":\n",
    "            return HDPHMM._resample_auxiliary_transition_atom_complete(\n",
    "                alpha,\n",
    "                beta[state2],\n",
    "                n_initial[state2] + n_transition[state1][state2],\n",
    "                use_approximation,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"resample_type must be either mh or complete\")\n",
    "\n",
    "    # TODO: decide whether to use either MH resampling or approximation sampling and\n",
    "    # remove the alternative, unnecessary complexity in code\n",
    "    def _resample_auxiliary_transition_variables(\n",
    "        self, ncores=1, resample_type=\"mh\", use_approximation=True\n",
    "    ):\n",
    "        # standard process uses typical list comprehension\n",
    "        if ncores < 2:\n",
    "            self.auxiliary_transition_variables = {\n",
    "                s1: {\n",
    "                    s2: HDPHMM._resample_auxiliary_transition_atom(\n",
    "                        (s1, s2),\n",
    "                        alpha=self.hyperparameters[\"alpha\"],\n",
    "                        beta=self.beta_transition,\n",
    "                        n_initial=self.n_initial,\n",
    "                        n_transition=self.n_transition,\n",
    "                        auxiliary_transition_variables=self.auxiliary_transition_variables,\n",
    "                        resample_type=resample_type,\n",
    "                        use_approximation=use_approximation,\n",
    "                    )\n",
    "                    for s2 in self.states\n",
    "                }\n",
    "                for s1 in self.states\n",
    "            }\n",
    "\n",
    "        # parallel process uses anonymous functions and mapping\n",
    "        else:\n",
    "            # specify ordering of states\n",
    "            state_pairs = [(s1, s2) for s1 in self.states for s2 in self.states]\n",
    "\n",
    "            # parallel process resamples\n",
    "            resample_partial = functools.partial(\n",
    "                HDPHMM._resample_auxiliary_transition_atom,\n",
    "                alpha=self.hyperparameters[\"alpha\"],\n",
    "                beta=self.beta_transition,\n",
    "                n_initial=self.n_initial,\n",
    "                n_transition=self.n_transition,\n",
    "                auxiliary_transition_variables=self.auxiliary_transition_variables,\n",
    "                resample_type=resample_type,\n",
    "                use_approximation=use_approximation,\n",
    "            )\n",
    "            pool = multiprocessing.Pool(processes=ncores)\n",
    "            auxiliary_transition_variables = pool.map(resample_partial, state_pairs)\n",
    "            pool.close()\n",
    "\n",
    "            # store as dictionary\n",
    "            for pair_n in range(len(state_pairs)):\n",
    "                state1, state2 = state_pairs[pair_n]\n",
    "                self.auxiliary_transition_variables[state1][\n",
    "                    state2\n",
    "                ] = auxiliary_transition_variables[pair_n]\n",
    "\n",
    "    def _get_beta_transition_metaparameters(self):\n",
    "        \"\"\"\n",
    "        Calculate parameters for the Dirichlet posterior of the transition beta\n",
    "        variables (with infinite states aggregated into 'None' state)\n",
    "        :return: dict, with a key for each state and None, and values equal to parameter\n",
    "        values\n",
    "        \"\"\"\n",
    "        # aggregate\n",
    "        params = {\n",
    "            s2: sum(self.auxiliary_transition_variables[s1][s2] for s1 in self.states)\n",
    "            for s2 in self.states\n",
    "        }\n",
    "        params[None] = self.hyperparameters[\"gamma\"]\n",
    "        return params\n",
    "\n",
    "    def resample_beta_transition(\n",
    "        self, ncores=1, auxiliary_resample_type=\"mh\", use_approximation=True, eps=1e-12\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Resample the beta values used to calculate the starting and transition\n",
    "        probabilities.\n",
    "        :param ncores: int. Number of cores to use in multithreading. Values below 2\n",
    "        mean the resampling step is not parallelised.\n",
    "        :param auxiliary_resample_type: either \"mh\" or \"complete\". Impacts the way\n",
    "        in which the auxiliary transition variables are estimated.\n",
    "        :param use_approximation: bool, flag to indicate whether an approximate\n",
    "        resampling should occur. ignored if `auxiliary_resample_type` is \"mh\"\n",
    "        :param eps: shrinkage parameter to avoid rounding error.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # auxiliary variables must be resampled to resample beta variables\n",
    "        self._resample_auxiliary_transition_variables(\n",
    "            ncores=ncores,\n",
    "            resample_type=auxiliary_resample_type,\n",
    "            use_approximation=use_approximation,\n",
    "        )\n",
    "\n",
    "        # resample from Dirichlet posterior\n",
    "        params = self._get_beta_transition_metaparameters()\n",
    "        temp_result = np.random.dirichlet(list(params.values())).tolist()\n",
    "        beta_transition = dict(zip(list(params.keys()), temp_result))\n",
    "        self.beta_transition = shrink_probabilities(beta_transition, eps)\n",
    "\n",
    "    def calculate_beta_transition_loglikelihood(self):\n",
    "        # get Dirichlet hyperparameters\n",
    "        params = self._get_beta_transition_metaparameters()\n",
    "        ll_transition = np.log(\n",
    "            stats.dirichlet.pdf(\n",
    "                [self.beta_transition[s] for s in params.keys()],\n",
    "                [params[s] for s in params.keys()],\n",
    "            )\n",
    "        )\n",
    "        return ll_transition\n",
    "\n",
    "    def _get_beta_emission_metaparameters(self):\n",
    "        \"\"\"\n",
    "        Calculate parameters for the Dirichlet posterior of the emission beta variables\n",
    "        (with infinite states aggregated into 'None' state)\n",
    "        :return: dict, with a key for each emission, and values equal to parameter\n",
    "        values\n",
    "        \"\"\"\n",
    "        # aggregate\n",
    "        params = {\n",
    "            e: sum(self.n_emission[s][e] for s in self.states)\n",
    "            + self.hyperparameters[\"gamma_emission\"] / self.n\n",
    "            for e in self.emissions\n",
    "        }\n",
    "        return params\n",
    "\n",
    "    def resample_beta_emission(self, eps=1e-12):\n",
    "        \"\"\"\n",
    "        Resample the beta values used to calculate the emission probabilties.\n",
    "        :param eps: Minimum value for expected value before resampling.\n",
    "        :return: None.\n",
    "        \"\"\"\n",
    "        # resample from Dirichlet posterior\n",
    "        params = self._get_beta_emission_metaparameters()\n",
    "        temp_result = np.random.dirichlet(list(params.values())).tolist()\n",
    "        beta_emission = dict(zip(list(params.keys()), temp_result))\n",
    "        self.beta_emission = shrink_probabilities(beta_emission, eps)\n",
    "\n",
    "    def calculate_beta_emission_loglikelihood(self):\n",
    "        # get Dirichlet hyperparameters\n",
    "        params = self._get_beta_emission_metaparameters()\n",
    "        ll_emission = np.log(\n",
    "            stats.dirichlet.pdf(\n",
    "                [self.beta_emission[e] for e in self.emissions],\n",
    "                [params[e] for e in self.emissions],\n",
    "            )\n",
    "        )\n",
    "        return ll_emission\n",
    "\n",
    "    def _get_p_initial_metaparameters(self):\n",
    "        params = {\n",
    "            s: self.n_initial[s]\n",
    "            + self.hyperparameters[\"alpha\"] * self.beta_transition[s]\n",
    "            for s in self.states\n",
    "        }\n",
    "        params[None] = self.hyperparameters[\"alpha\"] * self.beta_transition[None]\n",
    "        return params\n",
    "\n",
    "    def resample_p_initial(self, eps=1e-12):\n",
    "        \"\"\"\n",
    "        Resample the starting probabilities. Performed as a sample from the posterior\n",
    "        distribution, which is a Dirichlet with pseudocounts and actual counts combined.\n",
    "        :param eps: minimum expected value.\n",
    "        :return: None.\n",
    "        \"\"\"\n",
    "        params = self._get_p_initial_metaparameters()\n",
    "        temp_result = np.random.dirichlet(list(params.values())).tolist()\n",
    "        p_initial = dict(zip(list(params.keys()), temp_result))\n",
    "        self.p_initial = shrink_probabilities(p_initial, eps)\n",
    "\n",
    "    def calculate_p_initial_loglikelihood(self):\n",
    "        params = self._get_p_initial_metaparameters()\n",
    "        ll_initial = np.log(\n",
    "            stats.dirichlet.pdf(\n",
    "                [self.p_initial[s] for s in params.keys()],\n",
    "                [params[s] for s in params.keys()],\n",
    "            )\n",
    "        )\n",
    "        return ll_initial\n",
    "\n",
    "    def _get_p_transition_metaparameters(self, state):\n",
    "        if self.sticky:\n",
    "            params = {\n",
    "                s2: self.n_transition[state][s2]\n",
    "                + self.hyperparameters[\"alpha\"]\n",
    "                * (1 - self.hyperparameters[\"kappa\"])\n",
    "                * self.beta_transition[s2]\n",
    "                for s2 in self.states\n",
    "            }\n",
    "            params[None] = (\n",
    "                self.hyperparameters[\"alpha\"]\n",
    "                * (1 - self.hyperparameters[\"kappa\"])\n",
    "                * self.beta_transition[None]\n",
    "            )\n",
    "            params[state] += (\n",
    "                self.hyperparameters[\"alpha\"] * self.hyperparameters[\"kappa\"]\n",
    "            )\n",
    "        else:\n",
    "            params = {\n",
    "                s2: self.n_transition[state][s2]\n",
    "                + self.hyperparameters[\"alpha\"] * self.beta_transition[s2]\n",
    "                for s2 in self.states\n",
    "            }\n",
    "            params[None] = self.hyperparameters[\"alpha\"] * self.beta_transition[None]\n",
    "\n",
    "        return params\n",
    "\n",
    "    def resample_p_transition(self, eps=1e-12):\n",
    "        \"\"\"\n",
    "        Resample the transition probabilities from the current beta values and kappa\n",
    "        value, if the chain is sticky.\n",
    "        :param eps: minimum expected value passed to Dirichlet sampling step\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # empty current transition values\n",
    "        self.p_transition = {}\n",
    "\n",
    "        # refresh each state in turn\n",
    "        for state in self.states:\n",
    "            params = self._get_p_transition_metaparameters(state)\n",
    "            temp_result = np.random.dirichlet(list(params.values())).tolist()\n",
    "            p_transition_state = dict(zip(list(params.keys()), temp_result))\n",
    "            self.p_transition[state] = shrink_probabilities(p_transition_state, eps)\n",
    "\n",
    "        # add transition probabilities from unseen states\n",
    "        # note: no stickiness update because these are aggregated states\n",
    "        params = {\n",
    "            k: self.hyperparameters[\"alpha\"] * v\n",
    "            for k, v in self.beta_transition.items()\n",
    "        }\n",
    "        temp_result = np.random.dirichlet(list(params.values())).tolist()\n",
    "        p_transition_none = dict(zip(list(params.keys()), temp_result))\n",
    "        self.p_transition[None] = shrink_probabilities(p_transition_none, eps)\n",
    "\n",
    "    def calculate_p_transition_loglikelihood(self):\n",
    "        \"\"\"\n",
    "        Note: this calculates the likelihood over all entries in the transition matrix.\n",
    "        If chains have been resampled (this is the case during MCMC sampling, for\n",
    "        example), then there may be entries in the transition matrix that no longer\n",
    "        correspond to actual states.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ll_transition = 0\n",
    "        states = self.p_transition.keys()\n",
    "\n",
    "        # get probability for each state\n",
    "        for state in states:\n",
    "            params = self._get_p_transition_metaparameters(state)\n",
    "            ll_transition += np.log(\n",
    "                stats.dirichlet.pdf(\n",
    "                    [self.p_transition[state][s] for s in states],\n",
    "                    [params[s] for s in states],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # get probability for aggregate state\n",
    "        params = {\n",
    "            k: self.hyperparameters[\"alpha\"] * v\n",
    "            for k, v in self.beta_transition.items()\n",
    "        }\n",
    "        ll_transition += np.log(\n",
    "            stats.dirichlet.pdf(\n",
    "                [self.p_transition[None][s] for s in states],\n",
    "                [params[s] for s in states],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return ll_transition\n",
    "\n",
    "    def _get_p_emission_metaparameters(self, state):\n",
    "        params = {\n",
    "            e: self.n_emission[state][e]\n",
    "            + self.hyperparameters[\"alpha_emission\"] * self.beta_emission[e]\n",
    "            for e in self.emissions\n",
    "        }\n",
    "        return params\n",
    "\n",
    "    def resample_p_emission(self, eps=1e-12):\n",
    "        \"\"\"\n",
    "        resample emission parameters from emission priors and counts.\n",
    "        :param eps: minimum expected value passed to Dirichlet distribution\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # find hyperparameters\n",
    "        for state in self.states:\n",
    "            params = self._get_p_emission_metaparameters(state)\n",
    "            temp_result = np.random.dirichlet(list(params.values())).tolist()\n",
    "            p_emission_state = dict(zip(list(params.keys()), temp_result))\n",
    "            self.p_emission[state] = shrink_probabilities(p_emission_state, eps)\n",
    "\n",
    "        # add emission probabilities from unseen states\n",
    "        params = {\n",
    "            k: self.hyperparameters[\"alpha_emission\"] * v\n",
    "            for k, v in self.beta_emission.items()\n",
    "        }\n",
    "        temp_result = np.random.dirichlet(list(params.values())).tolist()\n",
    "        p_emission_none = dict(zip(list(params.keys()), temp_result))\n",
    "        self.p_emission[None] = shrink_probabilities(p_emission_none, eps)\n",
    "\n",
    "    def calculate_p_emission_loglikelihood(self):\n",
    "        ll_emission = 0\n",
    "\n",
    "        # get probability for each state\n",
    "        for state in self.states:\n",
    "            params = self._get_p_emission_metaparameters(state)\n",
    "            ll_emission += np.log(\n",
    "                stats.dirichlet.pdf(\n",
    "                    [self.p_emission[state][e] for e in self.emissions],\n",
    "                    [params[e] for e in self.emissions],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # get probability for aggregate state\n",
    "        params = {\n",
    "            k: self.hyperparameters[\"alpha_emission\"] * v\n",
    "            for k, v in self.beta_emission.items()\n",
    "        }\n",
    "        ll_emission += np.log(\n",
    "            stats.dirichlet.pdf(\n",
    "                [self.p_emission[None][e] for e in self.emissions],\n",
    "                [params[e] for e in self.emissions],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return ll_emission\n",
    "\n",
    "    def print_fit_parameters(self):\n",
    "        \"\"\"\n",
    "        Prints a copy of the current state counts.\n",
    "        Used for convenient checking in a command line environment.\n",
    "        For dictionaries containing the raw values, use the `n_*` attributes.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # create copies to avoid editing\n",
    "        n_initial = copy.deepcopy(self.n_initial)\n",
    "        n_emission = copy.deepcopy(self.n_emission)\n",
    "        n_transition = copy.deepcopy(self.n_transition)\n",
    "\n",
    "        # make nested lists for clean printing\n",
    "        initial = [[str(s)] + [str(n_initial[s])] for s in self.states]\n",
    "        initial.insert(0, [\"S_i\", \"Y_0\"])\n",
    "        emissions = [\n",
    "            [str(s)] + [str(n_emission[s][e]) for e in self.emissions]\n",
    "            for s in self.states\n",
    "        ]\n",
    "        emissions.insert(0, [\"S_i \\\\ E_i\"] + list(map(str, self.emissions)))\n",
    "        transitions = [\n",
    "            [str(s1)] + [str(n_transition[s1][s2]) for s2 in self.states]\n",
    "            for s1 in self.states\n",
    "        ]\n",
    "        transitions.insert(0, [\"S_i \\\\ S_j\"] + list(map(lambda x: str(x), self.states)))\n",
    "\n",
    "        # format tables\n",
    "        ti = terminaltables.DoubleTable(initial, \"Starting state counts\")\n",
    "        te = terminaltables.DoubleTable(emissions, \"Emission counts\")\n",
    "        tt = terminaltables.DoubleTable(transitions, \"Transition counts\")\n",
    "        ti.padding_left = 1\n",
    "        ti.padding_right = 1\n",
    "        te.padding_left = 1\n",
    "        te.padding_right = 1\n",
    "        tt.padding_left = 1\n",
    "        tt.padding_right = 1\n",
    "        ti.justify_columns[0] = \"right\"\n",
    "        te.justify_columns[0] = \"right\"\n",
    "        tt.justify_columns[0] = \"right\"\n",
    "\n",
    "        # print tables\n",
    "        print(\"\\n\")\n",
    "        print(ti.table)\n",
    "        print(\"\\n\")\n",
    "        print(te.table)\n",
    "        print(\"\\n\")\n",
    "        print(tt.table)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        #\n",
    "        return None\n",
    "\n",
    "    def print_probabilities(self):\n",
    "        \"\"\"\n",
    "        Prints a copy of the current probabilities.\n",
    "        Used for convenient checking in a command line environment.\n",
    "        For dictionaries containing the raw values, use the `p_*` attributes.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # create copies to avoid editing\n",
    "        p_initial = copy.deepcopy(self.p_initial)\n",
    "        p_emission = copy.deepcopy(self.p_emission)\n",
    "        p_transition = copy.deepcopy(self.p_transition)\n",
    "\n",
    "        # convert to nested lists for clean printing\n",
    "        p_initial = [[str(s)] + [str(round(p_initial[s], 3))] for s in self.states]\n",
    "        p_emission = [\n",
    "            [str(s)] + [str(round(p_emission[s][e], 3)) for e in self.emissions]\n",
    "            for s in self.states\n",
    "        ]\n",
    "        p_transition = [\n",
    "            [str(s1)] + [str(round(p_transition[s1][s2], 3)) for s2 in self.states]\n",
    "            for s1 in self.states\n",
    "        ]\n",
    "        p_initial.insert(0, [\"S_i\", \"Y_0\"])\n",
    "        p_emission.insert(0, [\"S_i \\\\ E_j\"] + [str(e) for e in self.emissions])\n",
    "        p_transition.insert(0, [\"S_i \\\\ E_j\"] + [str(s) for s in self.states])\n",
    "\n",
    "        # format tables\n",
    "        ti = terminaltables.DoubleTable(p_initial, \"Starting state probabilities\")\n",
    "        te = terminaltables.DoubleTable(p_emission, \"Emission probabilities\")\n",
    "        tt = terminaltables.DoubleTable(p_transition, \"Transition probabilities\")\n",
    "        te.padding_left = 1\n",
    "        te.padding_right = 1\n",
    "        tt.padding_left = 1\n",
    "        tt.padding_right = 1\n",
    "        te.justify_columns[0] = \"right\"\n",
    "        tt.justify_columns[0] = \"right\"\n",
    "\n",
    "        # print tables\n",
    "        print(\"\\n\")\n",
    "        print(ti.table)\n",
    "        print(\"\\n\")\n",
    "        print(te.table)\n",
    "        print(\"\\n\")\n",
    "        print(tt.table)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        #\n",
    "        return None\n",
    "\n",
    "    def calculate_chain_loglikelihood(self):\n",
    "        \"\"\"\n",
    "        Calculate the negative log likelihood of the chain, given its current\n",
    "        latent states. This is calculated based on the observed emission sequences only,\n",
    "        and not on the probabilities of the hyperparameters.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return sum(\n",
    "            chain.neglogp_chain(self.p_initial, self.p_emission, self.p_transition)\n",
    "            for chain in self.chains\n",
    "        )\n",
    "\n",
    "    def calculate_loglikelihood(self):\n",
    "        \"\"\"\n",
    "        Negative log-likelihood of the entire HDPHMM object. Combines the likelihoods of\n",
    "        the transition and emission beta parameters, and of the chains themselves.\n",
    "        Does not include the probabilities of the hyperparameter priors.\n",
    "        :return: non-negative float\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.calculate_beta_transition_loglikelihood()\n",
    "            + self.calculate_beta_emission_loglikelihood()\n",
    "            + self.calculate_p_initial_loglikelihood()\n",
    "            + self.calculate_p_transition_loglikelihood()\n",
    "            + self.calculate_p_emission_loglikelihood()\n",
    "            + self.calculate_chain_loglikelihood()\n",
    "        )\n",
    "\n",
    "    def resample_chains(self, ncores=1):\n",
    "        \"\"\"\n",
    "        Resample the latent states in all chains. This uses Beam sampling to improve the\n",
    "        resampling time.\n",
    "        :param ncores: int, number of threads to use in multithreading.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # extract probabilities\n",
    "        p_initial, p_emission, p_transition = (\n",
    "            self.p_initial,\n",
    "            self.p_emission,\n",
    "            self.p_transition,\n",
    "        )\n",
    "\n",
    "        # create temporary function for mapping\n",
    "        resample_partial = functools.partial(\n",
    "            Chain.resample_latent_sequence,\n",
    "            states=list(self.states) + [None],\n",
    "            p_initial=copy.deepcopy(p_initial),\n",
    "            p_emission=copy.deepcopy(p_emission),\n",
    "            p_transition=copy.deepcopy(p_transition),\n",
    "        )\n",
    "\n",
    "        # parallel process resamples\n",
    "        pool = multiprocessing.Pool(processes=ncores)\n",
    "        new_latent_sequences = pool.map(\n",
    "            resample_partial,\n",
    "            ((chain.emission_sequence, chain.latent_sequence) for chain in self.chains),\n",
    "        )\n",
    "        pool.close()\n",
    "\n",
    "        # assign returned latent sequences back to Chains\n",
    "        for i in range(self.c):\n",
    "            self.chains[i].latent_sequence = new_latent_sequences[i]\n",
    "\n",
    "        # update chains using results\n",
    "        # TODO: parameter check if we should be using alpha or gamma as parameter\n",
    "        state_generator = dirichlet_process_generator(\n",
    "            self.hyperparameters[\"gamma\"], output_generator=self.state_generator()\n",
    "        )\n",
    "        for chain in self.chains:\n",
    "            chain.latent_sequence = [\n",
    "                s if s is not None else next(state_generator)\n",
    "                for s in chain.latent_sequence\n",
    "            ]\n",
    "\n",
    "        # update counts\n",
    "        self._n_update()\n",
    "\n",
    "    def maximise_hyperparameters(self):\n",
    "        \"\"\"\n",
    "        Choose the MAP (maximum a posteriori) value for the hyperparameters.\n",
    "        Not yet implemented.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"This has not yet been written!\"\n",
    "            + \" Ping the author if you want it to happen.\"\n",
    "        )\n",
    "        pass\n",
    "\n",
    "    def resample_hyperparameters(self):\n",
    "        \"\"\"\n",
    "        Resample hyperparameters using a Metropolis Hastings algorithm. Uses a\n",
    "        straightforward resampling approach, which (for each hyperparameter) samples a\n",
    "        proposed value according to the prior distribution, and accepts the proposed\n",
    "        value with probability scaled by the relative probabilities of the model under\n",
    "        the current and proposed model.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # iterate and accept each in order\n",
    "        for param_name in self.priors.keys():\n",
    "            # don't update kappa if not a sticky chain\n",
    "            if param_name == \"kappa\" and not self.sticky:\n",
    "                continue\n",
    "\n",
    "            # get current negative log likelihood\n",
    "            likelihood_curr = self.calculate_loglikelihood()\n",
    "\n",
    "            # log-likelihood under new parameter value\n",
    "            param_current = self.hyperparameters[param_name]\n",
    "            self.hyperparameters[param_name] = self.priors[param_name]()\n",
    "            likelihood_proposed = self.calculate_loglikelihood()\n",
    "\n",
    "            # find Metropolis Hasting acceptance probability\n",
    "            p_accept = min(1, np.exp(likelihood_proposed - likelihood_curr))\n",
    "\n",
    "            # choose whether to accept\n",
    "            alpha_accepted = bool(np.random.binomial(n=1, p=p_accept))\n",
    "\n",
    "            # if we do not accept, revert to the previous value\n",
    "            if not alpha_accepted:\n",
    "                self.hyperparameters[param_name] = param_current\n",
    "\n",
    "    def mcmc(self, n=1000, burn_in=500, save_every=10, ncores=1, verbose=True):\n",
    "        \"\"\"\n",
    "        Use Markov chain Monte Carlo to estimate the starting, transition, and emission\n",
    "        parameters of the HDPHMM, as well as the number of latent states.\n",
    "        :param n: int, number of iterations to complete.\n",
    "        :param burn_in: int, number of iterations to complete before savings results.\n",
    "        :param save_every: int, only iterations which are a multiple of `save_every`\n",
    "        will have their results appended to the results.\n",
    "        :param ncores: int, number of cores to use in multithreaded latent state\n",
    "        resampling.\n",
    "        :param verbose: bool, flag to indicate whether iteration-level statistics should\n",
    "        be printed.\n",
    "        :return: A dict containing results from every saved iteration. Includes:\n",
    "          + the number of states of the HDPHMM\n",
    "          + the negative log likelihood of the entire model\n",
    "          + the negative log likelihood of the chains only\n",
    "          + the hyperparameters of the HDPHMM\n",
    "          + the emission beta values\n",
    "          + the transition beta values\n",
    "          + all probability dictionary objects\n",
    "        \"\"\"\n",
    "        # store hyperparameters in a single dict\n",
    "        results = {\n",
    "            \"state_count\": list(),\n",
    "            \"loglikelihood\": list(),\n",
    "            \"chain_loglikelihood\": list(),\n",
    "            \"hyperparameters\": list(),\n",
    "            \"beta_emission\": list(),\n",
    "            \"beta_transition\": list(),\n",
    "            \"parameters\": list(),\n",
    "        }\n",
    "\n",
    "        for i in tqdm.tqdm(range(n)):\n",
    "            # update statistics\n",
    "            states_prev = copy.copy(self.states)\n",
    "\n",
    "            # work down hierarchy when resampling\n",
    "            self.update_states()\n",
    "            self.resample_hyperparameters()\n",
    "            self.resample_beta_transition(ncores=ncores)\n",
    "            self.resample_beta_emission()\n",
    "            self.resample_p_initial()\n",
    "            self.resample_p_transition()\n",
    "            self.resample_p_emission()\n",
    "            self.resample_chains(ncores=ncores)\n",
    "\n",
    "            # update computation-heavy statistics\n",
    "            likelihood_curr = self.calculate_loglikelihood()\n",
    "\n",
    "            # print iteration summary if required\n",
    "            if verbose:\n",
    "                if i == burn_in:\n",
    "                    tqdm.tqdm.write(\"Burn-in period complete\")\n",
    "                states_taken = states_prev - self.states\n",
    "                states_added = self.states - states_prev\n",
    "                msg = [\n",
    "                    \"Iter: {}\".format(i),\n",
    "                    \"Likelihood: {0:.1f}\".format(likelihood_curr),\n",
    "                    \"states: {}\".format(len(self.states)),\n",
    "                ]\n",
    "                if len(states_added) > 0:\n",
    "                    msg.append(\"states added: {}\".format(states_added))\n",
    "                if len(states_taken) > 0:\n",
    "                    msg.append(\"states removed: {}\".format(states_taken))\n",
    "                tqdm.tqdm.write(\", \".join(msg))\n",
    "\n",
    "            # store results\n",
    "            if i >= burn_in and i % save_every == 0:\n",
    "                # get hyperparameters as nested lists\n",
    "                p_initial = copy.deepcopy(self.p_initial)\n",
    "                p_emission = copy.deepcopy(self.p_emission)\n",
    "                p_transition = copy.deepcopy(self.p_transition)\n",
    "\n",
    "                # save new data\n",
    "                results[\"state_count\"].append(self.k)\n",
    "                results[\"loglikelihood\"].append(likelihood_curr)\n",
    "                results[\"chain_loglikelihood\"].append(\n",
    "                    self.calculate_chain_loglikelihood()\n",
    "                )\n",
    "                results[\"hyperparameters\"].append(copy.deepcopy(self.hyperparameters))\n",
    "                results[\"beta_emission\"].append(self.beta_emission)\n",
    "                results[\"beta_transition\"].append(self.beta_transition)\n",
    "                results[\"parameters\"].append(\n",
    "                    {\n",
    "                        \"p_initial\": p_initial,\n",
    "                        \"p_emission\": p_emission,\n",
    "                        \"p_transition\": p_transition,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # return saved observations\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = HDPHMM(emission_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf = h.state_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-347b2836e8ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-117-b450a33422b8>\u001b[0m in \u001b[0;36mstate_generator\u001b[0;34m(self, eps)\u001b[0m\n\u001b[1;32m    308\u001b[0m             temp_p_emission = np.random.dirichlet(\n\u001b[1;32m    309\u001b[0m                 [\n\u001b[0;32m--> 310\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"alpha\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_emission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memissions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m                 ]\n\u001b[1;32m    312\u001b[0m             )\n",
      "\u001b[0;32m<ipython-input-117-b450a33422b8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    308\u001b[0m             temp_p_emission = np.random.dirichlet(\n\u001b[1;32m    309\u001b[0m                 [\n\u001b[0;32m--> 310\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"alpha\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_emission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memissions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m                 ]\n\u001b[1;32m    312\u001b[0m             )\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
